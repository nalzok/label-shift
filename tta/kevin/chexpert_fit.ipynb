{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence WARNING:root:The use of `check_types` is deprecated and does not have any effect.\n",
    "# https://github.com/tensorflow/probability/issues/1523\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class CheckTypesFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return \"check_types\" not in record.getMessage()\n",
    "\n",
    "\n",
    "logger.addFilter(CheckTypesFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=3)\n",
    "import scipy.stats\n",
    "import einops\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "from time import time\n",
    "\n",
    "import chex\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, grad, jit, lax\n",
    "from jax import numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import flax\n",
    "\n",
    "import jaxopt\n",
    "import optax\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cpu_count = os.cpu_count()\n",
    "print(cpu_count)\n",
    "\n",
    "# Run jax on multiple CPU cores\n",
    "# https://github.com/google/jax/issues/5506\n",
    "# https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu\n",
    "import os \n",
    "#os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=90'\n",
    "\n",
    "import jax\n",
    "print(jax.devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 19:49:37.960510: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-10 19:49:37.993371: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-10 19:49:38.787683: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-10 19:49:38.787829: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-10 19:49:38.787840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'probml_utils.mlp_flax.MLPNetwork'>\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/probml/probml-utils\n",
    "import probml_utils\n",
    "from probml_utils.mlp_flax import MLPNetwork, NeuralNetClassifier\n",
    "print(MLPNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tta.utils.Dataset'>\n",
      "<class 'tta.datasets.MultipleDomainDataset'>\n",
      "<class 'tta.datasets.chexpert.MultipleDomainCheXpert'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tta\n",
    "from tta.utils import *\n",
    "print(Dataset)\n",
    "\n",
    "from tta.datasets import *\n",
    "print(MultipleDomainDataset)\n",
    "\n",
    "from tta.datasets.chexpert import *\n",
    "print(MultipleDomainCheXpert)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-computed data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X', 'YZ', 'columns']\n",
      "(139907, 1376)\n",
      "(139907, 4)\n",
      "['PNEUMONIA' 'EFFUSION' 'GENDER' 'AGE_QUANTIZED']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "root = '/home/kpmurphy/data/CheXpert'\n",
    "root = Path(root)\n",
    "\n",
    "data = np.load(root / 'data_matrix.npz', allow_pickle=True)\n",
    "print(data.files)\n",
    "print(data['X'].shape)\n",
    "print(data['YZ'].shape)\n",
    "print(data['columns'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit logistic regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111925, 27982]\n"
     ]
    }
   ],
   "source": [
    "X = data['X']\n",
    "ndx = np.where(data['columns'] == 'EFFUSION')[0][0]\n",
    "Y = np.array(data['YZ'][:,ndx], dtype=int)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "N_train  = X_train.shape[0]\n",
    "N_test  = X_test.shape[0]\n",
    "print([N_train, N_test])\n",
    "\n",
    "classifier = Pipeline([\n",
    "        ('standardscaler', StandardScaler()),\n",
    "        #('poly', PolynomialFeatures(degree=2)), \n",
    "        ('logreg', LogisticRegression(random_state=0, max_iter=500, C=10, solver='sag', multi_class='multinomial'))\n",
    "])\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, max_iter=500, C=10, solver='sag', multi_class='multinomial')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#N = 100\n",
    "N  = N_train\n",
    "XX = X_train[:N]\n",
    "YY = Y_train[:N]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier.fit(XX, YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6214352083482239\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs = classifier.predict_proba(X_test)\n",
    "\n",
    "y_pred = jnp.argmax(probs, axis=1)\n",
    "y_pred2 = classifier.predict(X_test)\n",
    "assert np.allclose(y_pred, y_pred2)\n",
    "\n",
    "y_true = Y_test\n",
    "acc = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "print(acc)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit logistic regression with flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111925, 27982]\n"
     ]
    }
   ],
   "source": [
    "X = data['X']\n",
    "ndx = np.where(data['columns'] == 'EFFUSION')[0][0]\n",
    "Y = np.array(data['YZ'][:,ndx], dtype=int)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "N_train  = X_train.shape[0]\n",
    "N_test  = X_test.shape[0]\n",
    "print([N_train, N_test])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N = 10000 # use subset of data\n",
    "N  = N_train # use all data\n",
    "\n",
    "XX = X_train[:N]\n",
    "nclasses = 2\n",
    "YY = Y_train[:N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhidden = () + (nclasses,) # set nhidden() to get logistic regression\n",
    "network = MLPNetwork(nhidden)\n",
    "key = jr.PRNGKey(0)\n",
    "opt = optax.adamw(1e-3) #'adam+warmup'\n",
    "mlp = NeuralNetClassifier(network, key, nclasses, l2reg=0, standardize=True,\n",
    "        batch_size=512, num_epochs=10, print_every=1, optimizer=opt)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit optax\n",
      "epoch 0, train loss 0.686, train accuracy 0.591\n",
      "epoch 1, train loss 0.671, train accuracy 0.604\n",
      "epoch 2, train loss 0.668, train accuracy 0.606\n",
      "epoch 3, train loss 0.668, train accuracy 0.607\n",
      "epoch 4, train loss 0.667, train accuracy 0.609\n",
      "epoch 5, train loss 0.666, train accuracy 0.609\n",
      "epoch 6, train loss 0.667, train accuracy 0.608\n",
      "epoch 7, train loss 0.666, train accuracy 0.608\n",
      "epoch 8, train loss 0.666, train accuracy 0.609\n",
      "epoch 9, train loss 0.666, train accuracy 0.609\n",
      "CPU times: user 2min 2s, sys: 8.82 s, total: 2min 11s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mlp.fit(XX, YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.6144293\n",
      "test accuracy 0.60042167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs = mlp.predict(X_train)\n",
    "y_pred = jnp.argmax(probs, axis=1)\n",
    "acc = jnp.mean(Y_train == y_pred)\n",
    "print('train accuracy', acc)\n",
    "\n",
    "probs = mlp.predict(X_test)\n",
    "y_pred = jnp.argmax(probs, axis=1)\n",
    "acc = jnp.mean(Y_test == y_pred)\n",
    "print('test accuracy', acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make shifted datasets for each domain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"/home/kpmurphy/data/CheXpert\")\n",
    "dataset_y_column = \"EFFUSION\"\n",
    "dataset_z_column = \"GENDER\"\n",
    "dataset_use_embedding = True\n",
    "train_domains_set = [9]\n",
    "target_domain_count = 512\n",
    "\n",
    "import random\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "key = jax.random.PRNGKey(seed)\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "\n",
    "dataset = MultipleDomainCheXpert(root, generator, dataset_y_column, dataset_z_column, dataset_use_embedding, train_domains_set, target_domain_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1aafb1a5b8a6c5cc9d9564fe8ce376ad7cec1976d94f450e8b79a35770c931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
