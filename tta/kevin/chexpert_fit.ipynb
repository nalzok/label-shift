{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=3)\n",
    "import scipy.stats\n",
    "import einops\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "from time import time\n",
    "\n",
    "import chex\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, grad, jit, lax\n",
    "from jax import numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import flax\n",
    "\n",
    "import jaxopt\n",
    "import optax\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cpu_count = os.cpu_count()\n",
    "print(cpu_count)\n",
    "\n",
    "# Run jax on multiple CPU cores\n",
    "# https://github.com/google/jax/issues/5506\n",
    "# https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu\n",
    "import os \n",
    "#os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=90'\n",
    "\n",
    "import jax\n",
    "print(jax.devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 19:09:50.715904: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-11 19:09:50.750871: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-11 19:09:51.498447: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-11 19:09:51.498617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-11-11 19:09:51.498628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'probml_utils.mlp_flax.MLPNetwork'>\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/probml/probml-utils\n",
    "import probml_utils\n",
    "from probml_utils.mlp_flax import MLPNetwork, NeuralNetClassifier\n",
    "print(MLPNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tta.utils.Dataset'>\n",
      "<class 'tta.datasets.MultipleDomainDataset'>\n",
      "<class 'tta.datasets.chexpert.MultipleDomainCheXpert'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tta\n",
    "from tta.utils import *\n",
    "print(Dataset)\n",
    "\n",
    "from tta.datasets import *\n",
    "print(MultipleDomainDataset)\n",
    "\n",
    "from tta.datasets.chexpert import *\n",
    "print(MultipleDomainCheXpert)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-computed data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features', 'attributes', 'columns', 'uniques']\n",
      "(190499, 1376) (190499, 19)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "root = '/home/kpmurphy/data/CheXpert'\n",
    "root = Path(root)\n",
    "\n",
    "data = np.load(root / 'data_matrix.npz', allow_pickle=True)\n",
    "print(data.files)\n",
    "print(data['features'].shape, data['attributes'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unknown gender =  1\n",
      "number of unknown disease =  34898\n",
      "removing  34898  rows\n",
      "(155601, 1376) (155601,) (155601,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "disease_ndx = np.where(data['columns'] == 'EFFUSION')[0][0]\n",
    "Yall = data['attributes'][:, disease_ndx]\n",
    "\n",
    "gender_ndx = np.where(data['columns'] == 'GENDER')[0][0]\n",
    "Zall = data['attributes'][:, gender_ndx]\n",
    "\n",
    "# diease labels\n",
    "# 0 = no mention  \n",
    "# 1 = positive   \n",
    "# 2 = uncertain  \n",
    "# 3 = negative  \n",
    "# gender: 0=female, 1= male, 2=unknown\n",
    "print('number of unknown gender = ', np.sum(Zall==2))\n",
    "print('number of unknown disease = ', np.sum( (Yall==0) | (Yall==2) ) )\n",
    "remove = (Yall==0) | (Yall==2) | (Zall==2)\n",
    "ndx = np.where(remove==True)[0]\n",
    "nremove = len(ndx)\n",
    "print('removing ', nremove, ' rows')\n",
    "keep = ~remove\n",
    "\n",
    "X = data['features'][keep]\n",
    "\n",
    "Y = Yall[keep]\n",
    "pos = (Y==1); neg = (Y==3)\n",
    "Y[pos]=1; Y[neg]=0\n",
    "\n",
    "Z  = Zall[keep]\n",
    "\n",
    "print(X.shape, Y.shape, Z.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit logistic regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111925, 27982]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "N_train  = X_train.shape[0]\n",
    "N_test  = X_test.shape[0]\n",
    "print([N_train, N_test])\n",
    "\n",
    "classifier = Pipeline([\n",
    "        ('standardscaler', StandardScaler()),\n",
    "        #('poly', PolynomialFeatures(degree=2)), \n",
    "        ('logreg', LogisticRegression(random_state=0, max_iter=500, C=10, solver='sag', multi_class='multinomial'))\n",
    "])\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, max_iter=500, C=10, solver='sag', multi_class='multinomial')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#N = 100\n",
    "N  = N_train\n",
    "XX = X_train[:N]\n",
    "YY = Y_train[:N]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier.fit(XX, YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6214352083482239\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs = classifier.predict_proba(X_test)\n",
    "\n",
    "y_pred = jnp.argmax(probs, axis=1)\n",
    "y_pred2 = classifier.predict(X_test)\n",
    "assert np.allclose(y_pred, y_pred2)\n",
    "\n",
    "y_true = Y_test\n",
    "acc = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "print(acc)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit logistic regression with flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124480, 31121]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "N_train  = X_train.shape[0]\n",
    "N_test  = X_test.shape[0]\n",
    "print([N_train, N_test])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N = 1000 # use subset of data\n",
    "N  = N_train # use all data\n",
    "\n",
    "XX = X_train[:N]\n",
    "nclasses = 2\n",
    "YY = Y_train[:N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhidden = () + (nclasses,) # set nhidden() to get logistic regression\n",
    "network = MLPNetwork(nhidden)\n",
    "key = jr.PRNGKey(0)\n",
    "opt = optax.adamw(1e-3) \n",
    "#opt = 'adam+warmup'\n",
    "mlp = NeuralNetClassifier(network, key, nclasses, l2reg=1e-3, standardize=False,\n",
    "        batch_size=512, num_epochs=30, print_every=5, optimizer=opt)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 0.493, train accuracy 0.762\n",
      "epoch 5, train loss 0.460, train accuracy 0.785\n",
      "epoch 10, train loss 0.456, train accuracy 0.786\n",
      "epoch 15, train loss 0.456, train accuracy 0.787\n",
      "epoch 20, train loss 0.455, train accuracy 0.787\n",
      "epoch 25, train loss 0.455, train accuracy 0.788\n",
      "CPU times: user 5min 28s, sys: 1min, total: 6min 28s\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mlp.fit(XX, YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.7917176\n",
      "test accuracy 0.7879246\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs = mlp.predict(X_train)\n",
    "y_pred = jnp.argmax(probs, axis=1)\n",
    "acc = jnp.mean(Y_train == y_pred)\n",
    "print('train accuracy', acc)\n",
    "\n",
    "probs = mlp.predict(X_test)\n",
    "y_pred = jnp.argmax(probs, axis=1)\n",
    "acc = jnp.mean(Y_test == y_pred)\n",
    "print('test accuracy', acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make shifted datasets for each domain (WIP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"/home/kpmurphy/data/CheXpert\")\n",
    "dataset_y_column = \"EFFUSION\"\n",
    "dataset_z_column = \"GENDER\"\n",
    "dataset_use_embedding = True\n",
    "train_domains_set = [9]\n",
    "target_domain_count = 512\n",
    "\n",
    "import random\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "key = jax.random.PRNGKey(seed)\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "\n",
    "dataset = MultipleDomainCheXpert(root, generator, dataset_y_column, dataset_z_column, dataset_use_embedding, train_domains_set, target_domain_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1aafb1a5b8a6c5cc9d9564fe8ce376ad7cec1976d94f450e8b79a35770c931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
